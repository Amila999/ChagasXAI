{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import wfdb\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = \"./model/model.keras\"\n",
    "REQUIRED_LENGTH = 1000\n",
    "NUM_SAMPLES_TO_PLOT = 1\n",
    "TARGET_FS = 100\n",
    "SaMiTrop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dat_files(directory):\n",
    "    files = os.listdir(directory)\n",
    "    dat_files = [file for file in files if file.endswith('.dat')]\n",
    "    return dat_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hea_files(directory, dat_files,true_file,false_file):\n",
    "    true_files = []\n",
    "    false_files = []\n",
    "    \n",
    "    for dat_file in dat_files:\n",
    "        hea_file = dat_file.replace('.dat', '.hea')\n",
    "        hea_path = os.path.join(directory, hea_file)\n",
    "\n",
    "        if os.path.exists(hea_path):\n",
    "            with open(hea_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                \n",
    "                if \"Chagas label: True\" in content:\n",
    "                    true_files.append(dat_file)\n",
    "                elif \"Chagas label: False\" in content:\n",
    "                    false_files.append(dat_file)\n",
    "\n",
    "    with open(true_file, 'w') as true_file:\n",
    "        for file_name in true_files:\n",
    "            true_file.write(file_name.split('.dat')[0] + '\\n')\n",
    "    \n",
    "    with open(false_file, 'w') as false_file:\n",
    "        for file_name in false_files:\n",
    "            false_file.write(file_name.split('.dat')[0] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csv(input_csv, true_output, false_output):\n",
    "    with open(input_csv, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        with open(true_output, 'w', encoding='utf-8') as true_file, open(false_output, 'w', encoding='utf-8') as false_file:\n",
    "            for row in reader:\n",
    "                if row[reader.fieldnames[2]].strip().upper() == 'TRUE':\n",
    "                    true_file.write(row[reader.fieldnames[0]] + '\\n')\n",
    "                else:\n",
    "                    false_file.write(row[reader.fieldnames[0]] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_files(input_txt, directory, output_txt):\n",
    "    with open(input_txt, 'r', encoding='utf-8') as file:\n",
    "        filenames = [line.strip() for line in file] \n",
    "    \n",
    "    existing_files = [f for f in filenames if os.path.isfile(os.path.join(directory, f + '.dat'))]\n",
    "    \n",
    "    with open(output_txt, 'w', encoding='utf-8') as output_file:\n",
    "        for file in existing_files:\n",
    "            output_file.write(file + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    if not tf.io.gfile.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file {model_path} not found.\")\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path, compile=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_signal(original_signal, original_fs, target_fs=100):\n",
    "    if original_fs == target_fs:\n",
    "        return original_signal\n",
    "    fs_ratio = target_fs / original_fs\n",
    "    return signal.resample(original_signal, int(original_signal.shape[0] * fs_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_signal_length(signal, target_length):\n",
    "    \"\"\"Pad or truncate signal to target length\"\"\"\n",
    "    if signal.shape[0] < target_length:\n",
    "        return np.pad(signal, ((0, target_length - signal.shape[0]), (0, 0)), mode='constant')\n",
    "    return signal[:target_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper\n",
    "def get_header_file(record):\n",
    "    if not record.endswith('.hea'):\n",
    "        header_file = record + '.hea'\n",
    "    else:\n",
    "        header_file = record\n",
    "    return header_file\n",
    "\n",
    "def load_text(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        string = f.read()\n",
    "    return string\n",
    "\n",
    "def load_header(record):\n",
    "    header_file = get_header_file(record)\n",
    "    header = load_text(header_file)\n",
    "    return header\n",
    "\n",
    "def load_signals(record):\n",
    "    signal, fields = wfdb.rdsamp(record)\n",
    "    return signal, fields\n",
    "\n",
    "def get_variable(string, variable_name):\n",
    "    variable = ''\n",
    "    has_variable = False\n",
    "    for l in string.split('\\n'):\n",
    "        if l.startswith(variable_name):\n",
    "            variable = l[len(variable_name):].strip()\n",
    "            has_variable = True\n",
    "    return variable, has_variable\n",
    "\n",
    "def remove_extra_characters(x):\n",
    "    x = str(x)\n",
    "    x = x.replace('\"', '').replace(\"'\", \"\")\n",
    "    x = x.replace('(', '').replace(')', '').replace('[', '').replace(']', '').replace('{', '').replace('}', '')\n",
    "    x = x.replace(' ', '').replace('\\t', '')\n",
    "    x = x.strip()\n",
    "    return x\n",
    "\n",
    "def is_number(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "def sanitize_boolean_value(x):\n",
    "    x = remove_extra_characters(x)\n",
    "    if (is_number(x) and float(x)==0) or (remove_extra_characters(x).casefold() in ('false', 'f', 'no', 'n')):\n",
    "        return 0\n",
    "    elif (is_number(x) and float(x)==1) or (remove_extra_characters(x).casefold() in ('true', 't', 'yes', 'y')):\n",
    "        return 1\n",
    "    else:\n",
    "        return float('nan')\n",
    "\n",
    "def get_label(string, allow_missing=False):\n",
    "    label, has_label = get_variable(string, label_string)\n",
    "    if not has_label and not allow_missing:\n",
    "        raise Exception('No label is available: are you trying to load the labels from the held-out data?')\n",
    "    label = sanitize_boolean_value(label)\n",
    "    return label\n",
    "\n",
    "def load_label(record):\n",
    "    header = load_header(record)\n",
    "    label = get_label(header)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_predictions(directory,file_list, output_excel,truefalse):\n",
    "    results = []\n",
    "    count = 0;\n",
    "    \n",
    "    with open(file_list, \"r\") as f:\n",
    "        for line in f:\n",
    "            o_f = line.strip()\n",
    "            dat_file = directory + o_f\n",
    "            \n",
    "            # Load the ECG signal\n",
    "            ecg, text = load_signals(dat_file)\n",
    "            original_fs = int(text[\"fs\"])\n",
    "            original_signal = ecg\n",
    "            \n",
    "            # Process the signal\n",
    "            resampled_signal = resample_signal(original_signal, original_fs, TARGET_FS)\n",
    "            adjusted_signal = adjust_signal_length(resampled_signal, REQUIRED_LENGTH)\n",
    "            input_tensor = tf.convert_to_tensor(np.expand_dims(adjusted_signal, 0), dtype=tf.float32)\n",
    "            \n",
    "            # Make prediction\n",
    "            probability_output = model.predict(input_tensor)\n",
    "            \n",
    "            # Store the results in a list\n",
    "            results.append([o_f, probability_output[0][0]])\n",
    "            count = count+1\n",
    "            if count == 200 and not truefalse:\n",
    "                break\n",
    "    \n",
    "    # Convert results to a DataFrame and sort by prediction result in descending order\n",
    "    df = pd.DataFrame(results, columns=[\"File Name\", \"Prediction Result\"])\n",
    "    if truefalse:\n",
    "        df = df.sort_values(by=\"Prediction Result\", ascending=False)\n",
    "    else:\n",
    "        df = df.sort_values(by=\"Prediction Result\", ascending=True)\n",
    "    \n",
    "    # Save to an Excel file\n",
    "    df.to_excel(output_excel, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "if SaMiTrop:\n",
    "    directory = '../samitrop_processed/'\n",
    "    dat_files = get_dat_files(directory)\n",
    "    process_hea_files(directory, dat_files,'existing_true_files.txt','existing_false_files.txt')\n",
    "    \n",
    "else:\n",
    "    directory = '../GMC2025/code15_wfdb/'\n",
    "    chagas_csv_label = '../GMC2025/code15_hdf5/code15_chagas_labels.csv'\n",
    "    split_csv(chagas_csv_label, 'true_values.txt', 'false_values.txt')\n",
    "    check_files('true_values.txt', directory, 'existing_true_files.txt')\n",
    "    check_files('false_values.txt', directory, 'existing_false_files.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_save_predictions(directory,\"existing_true_files.txt\", \"true_file_predictions.xlsx\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_save_predictions(directory,\"existing_false_files.txt\", \"false_file_predictions.xlsx\",False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
