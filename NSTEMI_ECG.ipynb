{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy import signal\n",
    "import neurokit2 as nk\n",
    "import shap\n",
    "from cleverhans.tf2.attacks.projected_gradient_descent import projected_gradient_descent\n",
    "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method\n",
    "from scipy.signal import butter, filtfilt\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = \"./model/model.keras\"\n",
    "RECORD_PATH = \"../GMC2025/code15_wfdb/474396\"\n",
    "DATASET_DIR = '../GMC2025/code15_wfdb/'\n",
    "BALANCED_SAMPLES_DIR = \"./BalancedSamples/\"\n",
    "REQUIRED_LENGTH = 1000\n",
    "TARGET_FS = 100\n",
    "NUM_ECGS = 100\n",
    "SAVE_DIR = \"./results/\"\n",
    "BEAT_WINDOW = (0.2, 0.4)  # seconds around R-peak\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    if not tf.io.gfile.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file {model_path} not found.\")\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path, compile=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_signal(original_signal, original_fs, target_fs=100):\n",
    "    if original_fs == target_fs:\n",
    "        return original_signal\n",
    "    fs_ratio = target_fs / original_fs\n",
    "    return signal.resample(original_signal, int(original_signal.shape[0] * fs_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_signal_length(signal, target_length):\n",
    "    \"\"\"Pad or truncate signal to target length\"\"\"\n",
    "    if signal.shape[0] < target_length:\n",
    "        return np.pad(signal, ((0, target_length - signal.shape[0]), (0, 0)), mode='constant')\n",
    "    return signal[:target_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_conv_layer(model):\n",
    "    for layer in reversed(model.layers):\n",
    "        if isinstance(layer, tf.keras.layers.Conv1D):\n",
    "            return layer.name\n",
    "    raise ValueError(\"No convolutional layer found in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_cam(model, img_array, layer_name):\n",
    "    # Create a grad model that outputs the conv layer and the final probability output.\n",
    "    conv_layer = model.get_layer(layer_name)\n",
    "    \n",
    "    grad_model = tf.keras.models.Model(\n",
    "        inputs=model.input, \n",
    "        outputs=[conv_layer.output, model.output]\n",
    "    )\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, preds = grad_model(img_array)\n",
    "\n",
    "        class_output = preds[:, 0]\n",
    "\n",
    "        class_idx = 1 if class_output > 0.1 else 0  \n",
    "        grads = tape.gradient(class_output, conv_outputs)\n",
    "\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1))\n",
    "    conv_output = conv_outputs[0] * pooled_grads\n",
    "\n",
    "    heatmap = np.sum(conv_output.numpy(), axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap = (heatmap - np.min(heatmap)) / (np.ptp(heatmap) + 1e-8)\n",
    "    \n",
    "    return heatmap, class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ecg_with_gradcam(signal, heatmap, lead_names, fs, class_idx):\n",
    "    num_leads = signal.shape[1]\n",
    "    time = np.arange(signal.shape[0]) / fs\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = GridSpec(num_leads, 1, figure=fig)\n",
    "    heatmap_normalized = (heatmap - np.min(heatmap)) / (np.ptp(heatmap) + 1e-8)\n",
    "    for lead in range(num_leads):\n",
    "        ax = fig.add_subplot(gs[lead, 0])\n",
    "        vertical_offset = lead * 3\n",
    "        ax.plot(time, signal[:, lead] + vertical_offset, linewidth=1, color='black')\n",
    "        heatmap_extent = [time[0], time[-1], vertical_offset - 1.5, vertical_offset + 1.5]\n",
    "        ax.imshow(\n",
    "            np.expand_dims(heatmap_normalized, axis=0),\n",
    "            cmap='viridis',\n",
    "            aspect='auto',\n",
    "            extent=heatmap_extent,\n",
    "            alpha=0.4,\n",
    "            origin='lower'\n",
    "        )\n",
    "        ax.set_ylabel(lead_names[lead], rotation=0, ha='right', va='center')\n",
    "        ax.set_yticks([])\n",
    "        ax.set_ylim(vertical_offset - 2, vertical_offset + 2)\n",
    "        if lead != num_leads - 1:\n",
    "            ax.set_xticks([])\n",
    "    plt.suptitle(f\"Predicted Class: {class_idx}\\nGrad-CAM Heatmap Overlay\", y=0.92)\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_filter(signal, lowcut=0.5, highcut=49.9, fs=100, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper\n",
    "def get_header_file(record):\n",
    "    if not record.endswith('.hea'):\n",
    "        header_file = record + '.hea'\n",
    "    else:\n",
    "        header_file = record\n",
    "    return header_file\n",
    "\n",
    "def load_text(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        string = f.read()\n",
    "    return string\n",
    "\n",
    "def load_header(record):\n",
    "    header_file = get_header_file(record)\n",
    "    header = load_text(header_file)\n",
    "    return header\n",
    "\n",
    "def load_signals(record):\n",
    "    signal, fields = wfdb.rdsamp(record)\n",
    "    return signal, fields\n",
    "\n",
    "def get_variable(string, variable_name):\n",
    "    variable = ''\n",
    "    has_variable = False\n",
    "    for l in string.split('\\n'):\n",
    "        if l.startswith(variable_name):\n",
    "            variable = l[len(variable_name):].strip()\n",
    "            has_variable = True\n",
    "    return variable, has_variable\n",
    "\n",
    "def remove_extra_characters(x):\n",
    "    x = str(x)\n",
    "    x = x.replace('\"', '').replace(\"'\", \"\")\n",
    "    x = x.replace('(', '').replace(')', '').replace('[', '').replace(']', '').replace('{', '').replace('}', '')\n",
    "    x = x.replace(' ', '').replace('\\t', '')\n",
    "    x = x.strip()\n",
    "    return x\n",
    "\n",
    "def is_number(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "def sanitize_boolean_value(x):\n",
    "    x = remove_extra_characters(x)\n",
    "    if (is_number(x) and float(x)==0) or (remove_extra_characters(x).casefold() in ('false', 'f', 'no', 'n')):\n",
    "        return 0\n",
    "    elif (is_number(x) and float(x)==1) or (remove_extra_characters(x).casefold() in ('true', 't', 'yes', 'y')):\n",
    "        return 1\n",
    "    else:\n",
    "        return float('nan')\n",
    "\n",
    "def get_label(string, allow_missing=False):\n",
    "    label, has_label = get_variable(string, label_string)\n",
    "    if not has_label and not allow_missing:\n",
    "        raise Exception('No label is available: are you trying to load the labels from the held-out data?')\n",
    "    label = sanitize_boolean_value(label)\n",
    "    return label\n",
    "\n",
    "def load_label(record):\n",
    "    header = load_header(record)\n",
    "    label = get_label(header)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chagas_label(text_extracted_from_signal):\n",
    "    for comment in text_extracted_from_signal['comments']:\n",
    "        if comment.startswith('Chagas label:'):\n",
    "            label_str = comment.split(':')[-1].strip()  # Extract \"False\" or \"True\"\n",
    "            break\n",
    "    true_label = 1 if label_str == \"True\" else 0\n",
    "    return true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_record(RECORD_PATH):\n",
    "    try:\n",
    "        ecg , fields= load_signals(RECORD_PATH)\n",
    "        true_label = get_chagas_label(fields)\n",
    "        fs = int(fields[\"fs\"])\n",
    "\n",
    "        resampled_signal = resample_signal(ecg, fs, TARGET_FS)\n",
    "        adjusted_signal = adjust_signal_length(resampled_signal, REQUIRED_LENGTH)\n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(adjusted_signal, 0), dtype=tf.float32)\n",
    "        \n",
    "        return adjusted_signal, input_tensor, fields\n",
    "    except Exception as main_error:\n",
    "        print(f\"Critical error: {str(main_error)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(MODEL_PATH)\n",
    "conv_layer_name = find_last_conv_layer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_signal, input_tensor, fields = process_record(RECORD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_original, class_idx_original = compute_grad_cam(model, input_tensor, conv_layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Grad-CAM\n",
    "print(f\"Original Signal - Predicted Class: {class_idx_original}\")\n",
    "plot_ecg_with_gradcam(\n",
    "    adjusted_signal,\n",
    "    heatmap_original,\n",
    "    fields['sig_name'],\n",
    "    TARGET_FS,\n",
    "    class_idx=class_idx_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_records(dataset_path):\n",
    "    records = set()  # to avoid duplicates\n",
    "\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith(\".dat\"):\n",
    "            # Extract the record name (without the .dat extension)\n",
    "            record_name = filename[:-4]\n",
    "            records.add(record_name)\n",
    "\n",
    "    sorted_record_names = sorted(records) # To make the same background using random seed\n",
    "    \n",
    "    return list(sorted_record_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_background_samples(dataset_path, num_samples=100):\n",
    "    records = load_all_records(dataset_path)\n",
    "    sampled_records = np.random.choice(records, num_samples, replace=False)\n",
    "    background_samples = []\n",
    "    \n",
    "    for record in sampled_records:\n",
    "        record_path = os.path.join(DATASET_DIR, record)\n",
    "        signal, fields = load_signals(record_path)\n",
    "        original_fs = int(fields['fs'])  # Get true sampling rate\n",
    "\n",
    "        processed_signal = adjust_signal_length(\n",
    "            resample_signal(signal, original_fs, TARGET_FS),\n",
    "            REQUIRED_LENGTH)\n",
    "        \n",
    "        background_samples.append(processed_signal)\n",
    "    \n",
    "    return np.array(background_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogitModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        # Reconstruct the complete computation graph\n",
    "        self.input_layer = model.input\n",
    "        self.feature_extractor = tf.keras.Model(\n",
    "            inputs=model.input,\n",
    "            outputs=model.layers[-2].output\n",
    "        )\n",
    "        \n",
    "        # Create output layer with original weights (no activation)\n",
    "        self.final_dense = tf.keras.layers.Dense(\n",
    "            units=model.layers[-1].units,\n",
    "            activation=None,\n",
    "            use_bias=True,\n",
    "            kernel_initializer=tf.constant_initializer(model.layers[-1].kernel.numpy()),\n",
    "            bias_initializer=tf.constant_initializer(model.layers[-1].bias.numpy())\n",
    "        )\n",
    "        \n",
    "        # Full computation graph\n",
    "        self.logits = self.final_dense(self.feature_extractor(self.input_layer))\n",
    "        \n",
    "        # Create formal Model with defined inputs/outputs\n",
    "        self.logit_model = tf.keras.Model(\n",
    "            inputs=self.input_layer,\n",
    "            outputs=self.logits\n",
    "        )\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.logit_model(x)\n",
    "    \n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return self.logit_model.inputs\n",
    "    \n",
    "    @property\n",
    "    def outputs(self):\n",
    "        return self.logit_model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shap_explanations(model, adjusted_signal, lead_names, fs, class_idx):\n",
    "    # Load background data\n",
    "    background_samples = load_background_samples(BALANCED_SAMPLES_DIR)\n",
    "    background = np.array(background_samples).astype(np.float32)\n",
    "\n",
    "    wrapped_model = LogitModel(model)\n",
    "    \n",
    "    # Initialize SHAP GradientExplainer\n",
    "    explainer = shap.GradientExplainer(\n",
    "        wrapped_model,\n",
    "        background,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Prepare input signal\n",
    "    input_signal = adjusted_signal[np.newaxis, :, :].astype(np.float32)\n",
    "    \n",
    "    # Compute SHAP values\n",
    "    shap_values = explainer.shap_values(input_signal)\n",
    "    \n",
    "    # Process and plot\n",
    "    shap_heatmap = np.squeeze(shap_values[0])\n",
    "    shap_heatmap_normalized = (shap_heatmap - np.min(shap_heatmap)) / (np.ptp(shap_heatmap) + 1e-8)\n",
    "    plot_ecg_with_shap(adjusted_signal, shap_heatmap_normalized, lead_names, fs, class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ecg_with_shap(signal, heatmap, lead_names, fs, class_idx):\n",
    "    if heatmap.shape != signal.shape:\n",
    "        raise ValueError(f\"Heatmap shape {heatmap.shape} doesn't match signal {signal.shape}\")\n",
    "    \n",
    "    num_leads = signal.shape[1]\n",
    "    time = np.arange(signal.shape[0])/fs\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = GridSpec(num_leads, 1, figure=fig)\n",
    "    \n",
    "    for lead in range(num_leads):\n",
    "        ax = fig.add_subplot(gs[lead, 0])\n",
    "        offset = lead * 3\n",
    "        ax.plot(time, signal[:, lead] + offset, 'k', lw=1)\n",
    "        ax.imshow(np.expand_dims(heatmap[:, lead], 0),\n",
    "                  cmap='viridis',\n",
    "                  aspect='auto',\n",
    "                  extent=[time[0], time[-1], offset-2, offset+5],\n",
    "                  alpha=0.4,\n",
    "                  origin='lower')\n",
    "        ax.set(ylabel=lead_names[lead], yticks=[], ylim=[offset-2, offset+2])\n",
    "        if lead != num_leads-1:\n",
    "            ax.set_xticks([])\n",
    "    \n",
    "    plt.suptitle(f\"Class {class_idx} - SHAP Values\", y=0.95)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_shap_explanations(\n",
    "    model=model,\n",
    "    adjusted_signal=adjusted_signal,\n",
    "    lead_names=fields['sig_name'],\n",
    "    fs=TARGET_FS,\n",
    "    class_idx=class_idx_original\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22",
   "metadata": {},
   "source": [
    "Fast Gradient Signed Method (FGSM) attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_output = model.predict(input_tensor)\n",
    "\n",
    "print(\"Predicted probabilities:\", probability_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i in range(12):\n",
    "    plt.subplot(12,1,i+1)\n",
    "    plt.plot(np.asarray(adjusted_signal)[:,i], label= \"Original ECG\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_linear(grad, eps):\n",
    "    optimal_perturbation = tf.sign(grad)\n",
    "    optimal_perturbation = tf.stop_gradient(optimal_perturbation)\n",
    "    scaled_perturbation = tf.multiply(eps, optimal_perturbation)\n",
    "    return scaled_perturbation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26",
   "metadata": {},
   "source": [
    "PhysioBand Gradient Ascent : Custom method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pbga_attack(\n",
    "    model,input_tensor,\n",
    "    original_signal,\n",
    "    target_class, \n",
    "    max_iterations = 100, \n",
    "    eps = 0.01, \n",
    "    target_fs = TARGET_FS, \n",
    "    verbose = True):\n",
    "\n",
    "    adv_x = tf.identity(input_tensor)\n",
    "    \n",
    "    success = False\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        current_prob = model.predict(adv_x, verbose = 1 if verbose else 0)[0][0]\n",
    "        \n",
    "        # Early stopping if attack succeeds\n",
    "        if (target_class == 0 and current_prob < 0.01) or \\\n",
    "           (target_class == 1 and current_prob > 0.8):\n",
    "            if(verbose):\n",
    "                print(f\"Success at iteration {iteration}: Prob={current_prob:.4f}\")\n",
    "            success = True\n",
    "            break\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(adv_x)\n",
    "            pred = model(adv_x)\n",
    "            loss = tf.keras.losses.binary_crossentropy(\n",
    "                tf.constant([[target_class]], dtype=tf.float32), \n",
    "                pred\n",
    "            )\n",
    "            loss = -loss  # Gradient ascent\n",
    "\n",
    "        grads = tape.gradient(loss, adv_x)\n",
    "        perturbation = optimize_linear(grads, eps)\n",
    "\n",
    "        # Physiological filtering\n",
    "        perturbation_filtered = np.array([\n",
    "            bandpass_filter(perturbation[0,:,i], \n",
    "                          fs=target_fs, \n",
    "                          lowcut=0.5, \n",
    "                          highcut=49.9)\n",
    "            for i in range(12)\n",
    "        ])\n",
    "        perturbation_filtered = np.expand_dims(perturbation_filtered.T, 0)\n",
    "\n",
    "        adv_x = adv_x + perturbation_filtered\n",
    "        \n",
    "        # Maintain original signal range\n",
    "        if original_signal is not None:\n",
    "            adv_x = tf.clip_by_value(adv_x, np.min(original_signal), np.max(original_signal))\n",
    "\n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_pbga = pbga_attack(\n",
    "    model=model,\n",
    "    input_tensor=input_tensor,\n",
    "    original_signal=adjusted_signal,\n",
    "    target_class= 1 - class_idx_original,\n",
    "    eps=0.01,\n",
    "    max_iterations=100,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i in range(12):\n",
    "    plt.subplot(12,1,i+1)\n",
    "    plt.plot(np.asarray(input_tensor)[0,:,i], label= \"Original ECG\")\n",
    "    plt.plot(np.asarray(adv_pbga)[0,:,i], label= \"Adverserial ECG\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30",
   "metadata": {},
   "source": [
    "Attack Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the original model\n",
    "wrapped_model = LogitModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Binary Loss Function\n",
    "def binary_loss(labels, logits):\n",
    "    return tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=labels,  # Shape (1, 1)\n",
    "        logits=logits   # Shape (1, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = 1 - class_idx_original\n",
    "target_label_tf = tf.constant([[target_class]], dtype=tf.float32)\n",
    "\n",
    "eps = 0.1  # perturbation size\n",
    "alpha = 0.01  # step size\n",
    "steps = 50  # attack steps\n",
    "\n",
    "# Check gradient flow\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(input_tensor)\n",
    "    logits = wrapped_model(input_tensor)\n",
    "grads = tape.gradient(logits, input_tensor)\n",
    "grad_norm = tf.reduce_mean(tf.abs(grads)).numpy()\n",
    "print(f\"Gradient Norm: {grad_norm}\")  # Should not be too close to zero\n",
    "\n",
    "if grad_norm < 1e-6:\n",
    "    raise ValueError(\"Gradient vanishing! Check if the model is differentiable.\")\n",
    "\n",
    "# Generate adversarial examples\n",
    "\n",
    "# Basic Iterative Method\n",
    "adv_bim = projected_gradient_descent(\n",
    "    wrapped_model,  # Use wrapped model\n",
    "    input_tensor,\n",
    "    eps,\n",
    "    alpha,\n",
    "    steps,\n",
    "    norm=np.inf,\n",
    "    y=target_label_tf,\n",
    "    targeted=True,\n",
    "    loss_fn=binary_loss,  # Use custom binary loss\n",
    ")\n",
    "\n",
    "adv_fgsm = fast_gradient_method(\n",
    "    wrapped_model,  # Use wrapped logit model\n",
    "    input_tensor,\n",
    "    eps,\n",
    "    norm=np.inf,\n",
    "    y=target_label_tf,\n",
    "    targeted=True,\n",
    "    loss_fn=binary_loss\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(12):\n",
    "    plt.subplot(12, 1, i+1)\n",
    "    plt.plot(adjusted_signal[:, i], label=\"Original\", linewidth=1)\n",
    "    plt.plot(adv_bim.numpy()[0, :, i], label=\"BIM\", alpha=0.8)\n",
    "    plt.plot(adv_fgsm.numpy()[0, :, i], label=\"FGSM\", alpha=0.8)\n",
    "    plt.ylabel(fields['sig_name'][i], rotation=0, ha='right')\n",
    "    plt.yticks([])\n",
    "    if i != 11: plt.xticks([])\n",
    "    plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Predictions\n",
    "print(\"Original:\", model.predict(input_tensor))\n",
    "print(\"BIM:\", model.predict(adv_bim))\n",
    "print(\"FGSM:\", model.predict(adv_fgsm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_median_beat(signal_lead, sampling_rate):\n",
    "    \"\"\"Extract median beat with error handling\"\"\"\n",
    "    try:\n",
    "        cleaned = nk.ecg_clean(signal_lead, sampling_rate=sampling_rate)\n",
    "        peaks = nk.ecg_findpeaks(cleaned, sampling_rate=sampling_rate)\n",
    "        rpeaks = peaks[\"ECG_R_Peaks\"]\n",
    "        \n",
    "        if len(rpeaks) < 3:  # Need at least 3 beats for median\n",
    "            return None\n",
    "\n",
    "        # Extract beats with fixed window size\n",
    "        window_pre = int(BEAT_WINDOW[0] * sampling_rate)\n",
    "        window_post = int(BEAT_WINDOW[1] * sampling_rate)\n",
    "        beats = []\n",
    "        for r in rpeaks:\n",
    "            start = r - window_pre\n",
    "            end = r + window_post\n",
    "            if start >= 0 and end <= len(cleaned):\n",
    "                beats.append(cleaned[start:end])\n",
    "        \n",
    "        if len(beats) < 3:\n",
    "            return None\n",
    "            \n",
    "        # Align beats to fixed length\n",
    "        beat_length = window_pre + window_post\n",
    "        aligned_beats = np.array([b[:beat_length] for b in beats if len(b) >= beat_length])\n",
    "        if len(aligned_beats) == 0:\n",
    "            return None\n",
    "            \n",
    "        return np.median(aligned_beats, axis=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Beat extraction error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate adversarial examples dictionary\n",
    "attacks = {\n",
    "    \"pbga\" : adv_pbga,\n",
    "    \"bim\": adv_bim,\n",
    "    \"fgsm\" : adv_fgsm\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_leads_comparison(original_signal, attacks, fields, fs):\n",
    "    num_leads = original_signal.shape[1]\n",
    "    \n",
    "    for attack_name, adv_example in attacks.items():\n",
    "        adv_signal = adv_example.numpy()[0]  # Remove batch dimension\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 16))\n",
    "        fig.suptitle(f\"Beat Morphology Comparison: {attack_name.upper()} Attack\", y=0.99)\n",
    "        gs = GridSpec(4, 3)  # 4 rows, 3 columns for 12 leads\n",
    "\n",
    "        for lead_idx in range(num_leads):\n",
    "            ax = fig.add_subplot(gs[lead_idx//3, lead_idx%3])\n",
    "            lead_name = fields['sig_name'][lead_idx]\n",
    "            \n",
    "            try:\n",
    "                # Original beat\n",
    "                orig_beat = extract_median_beat(original_signal[:, lead_idx], fs)\n",
    "                t_orig = np.arange(len(orig_beat)) / fs\n",
    "                \n",
    "                # Adversarial beat\n",
    "                adv_beat = extract_median_beat(adv_signal[:, lead_idx], fs)\n",
    "                t_adv = np.arange(len(adv_beat)) / fs\n",
    "                \n",
    "                # Plot both beats\n",
    "                ax.plot(t_orig, orig_beat, label='Original', color='blue', lw=2)\n",
    "                ax.plot(t_adv, adv_beat, label='Adversarial ' + attack_name, color='red', alpha=0.8, lw=1.5)\n",
    "                \n",
    "                ax.set_title(lead_name)\n",
    "                ax.set_xlabel('Time (s)')\n",
    "                ax.grid(True)\n",
    "                if lead_idx == 0:  # Only show legend on first subplot\n",
    "                    ax.legend()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                ax.set_title(f\"{lead_name} - Error\")\n",
    "                ax.text(0.5, 0.5, \"Beat extraction failed\", \n",
    "                       ha='center', va='center', transform=ax.transAxes)\n",
    "                continue\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_leads_comparison(adjusted_signal, attacks, fields, TARGET_FS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ecg_batch(model, dataset_dir, num_ecgs=100, eps=0.1, alpha=0.01, steps=50):\n",
    "\n",
    "    all_records = load_all_records(dataset_dir)\n",
    "    \n",
    "    if len(all_records) < num_ecgs:\n",
    "        raise ValueError(f\"Only {len(all_records)} records available, need {num_ecgs}\")\n",
    "\n",
    "    signal_len = int((BEAT_WINDOW[0] + BEAT_WINDOW[1]) * TARGET_FS)\n",
    "\n",
    "    original_beats = np.full((num_ecgs, signal_len, 12), np.nan)\n",
    "    adversarial_pbga_beats = np.full((num_ecgs, signal_len, 12), np.nan)\n",
    "    adversarial_bim_beats = np.full((num_ecgs, signal_len, 12), np.nan)\n",
    "    adversarial_fgsm_beats = np.full((num_ecgs, signal_len, 12), np.nan)\n",
    "    \n",
    "    metadata = []\n",
    "    \n",
    "    time_vector = np.linspace(-BEAT_WINDOW[0], BEAT_WINDOW[1], signal_len)\n",
    "\n",
    "    processed_count = 0\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    wrapped_model = LogitModel(model)\n",
    "\n",
    "    while processed_count < num_ecgs:\n",
    "        if not all_records:\n",
    "            raise RuntimeError(\"Exhausted all records before reaching target count\")\n",
    "            \n",
    "        record = rng.choice(all_records, 1)[0]\n",
    "        all_records.remove(record)\n",
    "        \n",
    "        try:\n",
    "            adjusted_data = process_record(os.path.join(dataset_dir, record))\n",
    "            if adjusted_data is None:\n",
    "                continue\n",
    "                \n",
    "            adjusted_signal, input_tensor, fields = adjusted_data\n",
    "            chagas_label = get_chagas_label(fields)\n",
    "            \n",
    "            orig_prob = model.predict(input_tensor, verbose=0)[0][0]\n",
    "            target_class = 1 if orig_prob < 0.1 else 0\n",
    "            target_label_tf = tf.constant([[target_class]], dtype=tf.float32)\n",
    "\n",
    "            adv_pbga = pbga_attack(\n",
    "                model,\n",
    "                input_tensor,\n",
    "                adjusted_signal,\n",
    "                target_class, \n",
    "                max_iterations = 100, \n",
    "                eps = 0.01, \n",
    "                target_fs = TARGET_FS,\n",
    "                verbose = False\n",
    "            )\n",
    "\n",
    "            adv_bim = projected_gradient_descent(\n",
    "                wrapped_model,  # Use wrapped model\n",
    "                input_tensor,\n",
    "                eps,\n",
    "                alpha,\n",
    "                steps,\n",
    "                norm=np.inf,\n",
    "                y=target_label_tf,\n",
    "                targeted=True,\n",
    "                loss_fn=binary_loss,  # Use custom binary loss\n",
    "            )\n",
    "            \n",
    "            adv_fgsm = fast_gradient_method(\n",
    "                wrapped_model,  # Use wrapped logit model\n",
    "                input_tensor,\n",
    "                eps,\n",
    "                norm=np.inf,\n",
    "                y=target_label_tf,\n",
    "                targeted=True,\n",
    "                loss_fn=binary_loss\n",
    "            )\n",
    "\n",
    "            # Get predictions\n",
    "            adv_pbga_prob = model.predict(adv_pbga, verbose=0)[0][0]\n",
    "            adv_bim_prob = model.predict(adv_bim, verbose=0)[0][0]\n",
    "            adv_fgsm_prob = model.predict(adv_fgsm, verbose=0)[0][0]\n",
    "\n",
    "            # Process beats for each lead\n",
    "            for lead in range(12):\n",
    "                try:\n",
    "                    # Original beat\n",
    "                    orig_beat = extract_median_beat(adjusted_signal[:, lead], TARGET_FS)\n",
    "                    if orig_beat is not None:\n",
    "                        original_beats[processed_count, :len(orig_beat), lead] = orig_beat\n",
    "                        \n",
    "                    # PBGA beat\n",
    "                    pbga_lead = adv_pbga.numpy()[0, :, lead]  # Correct tensor indexing\n",
    "                    adv_pbga_beat = extract_median_beat(pbga_lead, TARGET_FS)\n",
    "                    if adv_pbga_beat is not None:\n",
    "                        adversarial_pbga_beats[processed_count, :len(adv_pbga_beat), lead] = adv_pbga_beat\n",
    "                    \n",
    "                    # BIM beat\n",
    "                    bim_lead = adv_bim.numpy()[0, :, lead]  # Correct tensor indexing\n",
    "                    adv_bim_beat = extract_median_beat(bim_lead, TARGET_FS)\n",
    "                    if adv_bim_beat is not None:\n",
    "                        adversarial_bim_beats[processed_count, :len(adv_bim_beat), lead] = adv_bim_beat\n",
    "                        \n",
    "                    # FGSM beat\n",
    "                    fgsm_lead = adv_fgsm.numpy()[0, :, lead]\n",
    "                    adv_fgsm_beat = extract_median_beat(fgsm_lead, TARGET_FS)\n",
    "                    if adv_fgsm_beat is not None:\n",
    "                        adversarial_fgsm_beats[processed_count, :len(adv_fgsm_beat), lead] = adv_fgsm_beat\n",
    "                        \n",
    "                except Exception as lead_error:\n",
    "                    print(f\"Lead {lead} error: {str(lead_error)}\")\n",
    "                    continue\n",
    "\n",
    "            # Track success for both attacks\n",
    "            pbga_success = (target_class == 0 and adv_pbga_prob < 0.1) or (target_class == 1 and adv_pbga_prob > 0.2)\n",
    "            bim_success = (target_class == 0 and adv_bim_prob < 0.1) or (target_class == 1 and adv_bim_prob > 0.2)\n",
    "            fgsm_success = (target_class == 0 and adv_fgsm_prob < 0.1) or (target_class == 1 and adv_fgsm_prob > 0.2)\n",
    "            \n",
    "            metadata.append({\n",
    "                'index': processed_count,\n",
    "                'record_name': record,\n",
    "                'chagas_label': int(chagas_label),\n",
    "                'original_prob': float(orig_prob),\n",
    "                'adversarial_pbga_prob': float(adv_pbga_prob),\n",
    "                'adversarial_bim_prob': float(adv_bim_prob),\n",
    "                'adversarial_fgsm_prob': float(adv_fgsm_prob),\n",
    "                'pbga_success': int(pbga_success),\n",
    "                'bim_success': int(bim_success),\n",
    "                'fgsm_success': int(fgsm_success)\n",
    "            })\n",
    "            \n",
    "            processed_count += 1\n",
    "            print(f\"Processed {processed_count}/{num_ecgs} Original Probability: {orig_prob} | PBGA Probability {adv_pbga_prob} | PBGA: {pbga_success} | BIM Probability {adv_bim_prob} | BIM: {bim_success} | FGSM Probability {adv_fgsm_prob} | FGSM: {fgsm_success}\\n\")\n",
    "\n",
    "        except Exception as ecg_error:\n",
    "            print(f\"Failed ECG {record}: {str(ecg_error)}\")\n",
    "            continue\n",
    "\n",
    "    # Save results\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    np.save(os.path.join(SAVE_DIR, 'time_vector.npy'), time_vector)\n",
    "    np.save(os.path.join(SAVE_DIR, 'original_beats.npy'), original_beats)\n",
    "    np.save(os.path.join(SAVE_DIR, 'adversarial_pbga_beats.npy'), adversarial_pbga_beats)\n",
    "    np.save(os.path.join(SAVE_DIR, 'adversarial_bim_beats.npy'), adversarial_bim_beats)\n",
    "    np.save(os.path.join(SAVE_DIR, 'adversarial_fgsm_beats.npy'), adversarial_fgsm_beats)\n",
    "    pd.DataFrame(metadata).to_csv(os.path.join(SAVE_DIR, 'metadata.csv'), index=False)\n",
    "    \n",
    "    #return original_beats, adversarial_bim_beats, adversarial_fgsm_beats, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_ecg_batch(model, BALANCED_SAMPLES_DIR, num_ecgs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vector = np.load('./results/time_vector.npy')\n",
    "original_beats = np.load('./results/original_beats.npy')\n",
    "adversarial_pbga_beats = np.load('./results/adversarial_pbga_beats.npy')\n",
    "adversarial_bim_beats = np.load('./results/adversarial_bim_beats.npy')\n",
    "adversarial_fgsm_beats = np.load('./results/adversarial_fgsm_beats.npy')\n",
    "metadata = pd.read_csv('./results/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_leads(\n",
    "    record_idx,\n",
    "    original_beats,\n",
    "    adversarial_pbga_beats=None,\n",
    "    adversarial_bim_beats=None,\n",
    "    adversarial_fgsm_beats=None,\n",
    "    leads=None,\n",
    "    same_plot=False\n",
    "):\n",
    "    # Standard lead names and indices\n",
    "    ALL_LEADS = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "    \n",
    "    # Validate leads to plot\n",
    "    if leads is None:\n",
    "        leads_to_plot = ALL_LEADS  # Plot all leads by default\n",
    "    else:\n",
    "        leads_to_plot = [lead for lead in leads if lead in ALL_LEADS]\n",
    "        if not leads_to_plot:\n",
    "            raise ValueError(\"No valid leads provided.\")\n",
    "    \n",
    "    # Get indices of leads to plot\n",
    "    lead_indices = [ALL_LEADS.index(lead) for lead in leads_to_plot]\n",
    "    num_leads = len(lead_indices)\n",
    "    \n",
    "    # Process record indices\n",
    "    if isinstance(record_idx, int):\n",
    "        record_indices = [record_idx]\n",
    "    else:\n",
    "        record_indices = record_idx\n",
    "    num_records = len(record_indices)\n",
    "    \n",
    "    # Configure adversarial beat types\n",
    "    ADVERSARIAL_TYPES = [\n",
    "        ('pbga', 'PBGA', 'r'),\n",
    "        ('bim', 'BIM', 'b'),\n",
    "        ('fgsm', 'FGSM', 'g')\n",
    "    ]\n",
    "    \n",
    "    # Initialize containers\n",
    "    beat_data = [original_beats]\n",
    "    beat_types = ['Original']\n",
    "    colors = ['k']  # Black for original\n",
    "    line_styles = ['-', '--', ':', '-.']\n",
    "    \n",
    "    for suffix, name, color in ADVERSARIAL_TYPES:\n",
    "        beat_var = locals().get(f'adversarial_{suffix}_beats')\n",
    "        if beat_var is not None:\n",
    "            beat_data.append(beat_var)\n",
    "            beat_types.append(name)\n",
    "            colors.append(color)\n",
    "    \n",
    "    num_beat_types = len(beat_data)\n",
    "    \n",
    "    # Create subplots with guaranteed 2D indexing\n",
    "    total_rows = num_records * num_leads\n",
    "    \n",
    "    if same_plot:\n",
    "        fig, axs = plt.subplots(\n",
    "            total_rows, 1, \n",
    "            figsize=(15, 3 * total_rows), \n",
    "            squeeze=False,  # Force 2D array\n",
    "        )\n",
    "    else:\n",
    "        fig, axs = plt.subplots(\n",
    "            total_rows, \n",
    "            num_beat_types, \n",
    "            figsize=(5 * num_beat_types, 3 * total_rows), \n",
    "            squeeze=False,  # Force 2D array\n",
    "        )\n",
    "    \n",
    "    # Set the suptitle to include all record indices\n",
    "    plt.suptitle(f\"ECG Leads Comparison (Records {', '.join(map(str, record_indices))})\",y=1)\n",
    "    \n",
    "    # Plot for each record and lead\n",
    "    for record_num, r_idx in enumerate(record_indices):\n",
    "        for lead_plot_num, lead_idx in enumerate(lead_indices):\n",
    "            lead_name = ALL_LEADS[lead_idx]\n",
    "            row = record_num * num_leads + lead_plot_num\n",
    "            \n",
    "            if same_plot:\n",
    "                ax = axs[row, 0]\n",
    "                for bt_idx in range(num_beat_types):\n",
    "                    ax.plot(\n",
    "                        time_vector,\n",
    "                        beat_data[bt_idx][r_idx, :, lead_idx],\n",
    "                        color=colors[bt_idx],\n",
    "                        linestyle=line_styles[bt_idx % len(line_styles)],\n",
    "                        label=beat_types[bt_idx]\n",
    "                    )\n",
    "                ax.set_title(f\"Record {r_idx}, Lead {lead_name}\")\n",
    "                ax.grid(True)\n",
    "                ax.legend()\n",
    "            else:\n",
    "                for bt_idx in range(num_beat_types):\n",
    "                    ax = axs[row, bt_idx]\n",
    "                    ax.plot(\n",
    "                        time_vector,\n",
    "                        beat_data[bt_idx][r_idx, :, lead_idx],\n",
    "                        color=colors[bt_idx]\n",
    "                    )\n",
    "                    ax.set_title(f\"Record {r_idx}, {beat_types[bt_idx]} (Lead {lead_name})\")\n",
    "                    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_leads(\n",
    "    record_idx=[12],\n",
    "    original_beats=original_beats,\n",
    "    adversarial_pbga_beats=adversarial_pbga_beats,\n",
    "    adversarial_bim_beats=adversarial_bim_beats,\n",
    "    adversarial_fgsm_beats=adversarial_fgsm_beats,\n",
    "    #leads=['V3'],\n",
    "    same_plot=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
