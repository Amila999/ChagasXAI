{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy import signal\n",
    "import neurokit2 as nk\n",
    "import shap\n",
    "from scipy.ndimage import uniform_filter\n",
    "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method\n",
    "from cleverhans.tf2.attacks.projected_gradient_descent import projected_gradient_descent\n",
    "from cleverhans.tf2.attacks.momentum_iterative_method import momentum_iterative_method\n",
    "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method\n",
    "from cleverhans.tf2.attacks.carlini_wagner_l2 import carlini_wagner_l2\n",
    "from cleverhans.tf2.attacks.madry_et_al import madry_et_al\n",
    "from scipy.signal import butter, filtfilt, lfilter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = \"./model/model.keras\"\n",
    "RECORD_PATH = \"./input/2767239\" #Positive Chagas\n",
    "#RECORD_PATH = \"./input/14\" #Negative Chagas\n",
    "DATASET_DIR = '../GMC2025/code15_wfdb/'\n",
    "REQUIRED_LENGTH = 1000\n",
    "NUM_SAMPLES_TO_PLOT = 1\n",
    "TARGET_FS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    if not tf.io.gfile.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file {model_path} not found.\")\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path, compile=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_signal(original_signal, original_fs, target_fs=100):\n",
    "    if original_fs == target_fs:\n",
    "        return original_signal\n",
    "    fs_ratio = target_fs / original_fs\n",
    "    return signal.resample(original_signal, int(original_signal.shape[0] * fs_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_signal_length(signal, target_length):\n",
    "    \"\"\"Pad or truncate signal to target length\"\"\"\n",
    "    if signal.shape[0] < target_length:\n",
    "        return np.pad(signal, ((0, target_length - signal.shape[0]), (0, 0)), mode='constant')\n",
    "    return signal[:target_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_conv_layer(model):\n",
    "    for layer in reversed(model.layers):\n",
    "        if isinstance(layer, tf.keras.layers.Conv1D):\n",
    "            return layer.name\n",
    "    raise ValueError(\"No convolutional layer found in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_cam(model, img_array, layer_name):\n",
    "    logits = model.layers[-2].output\n",
    "    \n",
    "    grad_model = tf.keras.models.Model(inputs=model.input, outputs=[model.get_layer(layer_name).output, logits])\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, preds = grad_model(img_array)\n",
    "        \n",
    "        \n",
    "        class_output = preds[:, 0]\n",
    "        class_idx = 1 if class_output > 0.5 else 0  # Threshold at 0.5\n",
    "        \n",
    "        grads = tape.gradient(class_output, conv_outputs)\n",
    "\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1))\n",
    "    conv_output = conv_outputs[0] * pooled_grads\n",
    "\n",
    "    heatmap = np.sum(conv_output.numpy(), axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap = (heatmap - np.min(heatmap)) / (np.ptp(heatmap) + 1e-8)\n",
    "    \n",
    "    return heatmap, class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ecg_with_gradcam(signal, heatmap, lead_names, fs, class_idx):\n",
    "    num_leads = signal.shape[1]\n",
    "    time = np.arange(signal.shape[0]) / fs\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = GridSpec(num_leads, 1, figure=fig)\n",
    "    heatmap_normalized = (heatmap - np.min(heatmap)) / (np.ptp(heatmap) + 1e-8)\n",
    "    for lead in range(num_leads):\n",
    "        ax = fig.add_subplot(gs[lead, 0])\n",
    "        vertical_offset = lead * 3\n",
    "        ax.plot(time, signal[:, lead] + vertical_offset, linewidth=1, color='black')\n",
    "        heatmap_extent = [time[0], time[-1], vertical_offset - 1.5, vertical_offset + 1.5]\n",
    "        ax.imshow(\n",
    "            np.expand_dims(heatmap_normalized, axis=0),\n",
    "            cmap='viridis',\n",
    "            aspect='auto',\n",
    "            extent=heatmap_extent,\n",
    "            alpha=0.4,\n",
    "            origin='lower'\n",
    "        )\n",
    "        ax.set_ylabel(lead_names[lead], rotation=0, ha='right', va='center')\n",
    "        ax.set_yticks([])\n",
    "        ax.set_ylim(vertical_offset - 2, vertical_offset + 2)\n",
    "        if lead != num_leads - 1:\n",
    "            ax.set_xticks([])\n",
    "    plt.suptitle(f\"Predicted Class: {class_idx}\\nGrad-CAM Heatmap Overlay\", y=0.92)\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_filter(signal, lowcut=0.5, highcut=49.9, fs=100, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper\n",
    "def get_header_file(record):\n",
    "    if not record.endswith('.hea'):\n",
    "        header_file = record + '.hea'\n",
    "    else:\n",
    "        header_file = record\n",
    "    return header_file\n",
    "\n",
    "def load_text(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        string = f.read()\n",
    "    return string\n",
    "\n",
    "def load_header(record):\n",
    "    header_file = get_header_file(record)\n",
    "    header = load_text(header_file)\n",
    "    return header\n",
    "\n",
    "def load_signals(record):\n",
    "    signal, fields = wfdb.rdsamp(record)\n",
    "    return signal, fields\n",
    "\n",
    "def get_variable(string, variable_name):\n",
    "    variable = ''\n",
    "    has_variable = False\n",
    "    for l in string.split('\\n'):\n",
    "        if l.startswith(variable_name):\n",
    "            variable = l[len(variable_name):].strip()\n",
    "            has_variable = True\n",
    "    return variable, has_variable\n",
    "\n",
    "def remove_extra_characters(x):\n",
    "    x = str(x)\n",
    "    x = x.replace('\"', '').replace(\"'\", \"\")\n",
    "    x = x.replace('(', '').replace(')', '').replace('[', '').replace(']', '').replace('{', '').replace('}', '')\n",
    "    x = x.replace(' ', '').replace('\\t', '')\n",
    "    x = x.strip()\n",
    "    return x\n",
    "\n",
    "def is_number(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "def sanitize_boolean_value(x):\n",
    "    x = remove_extra_characters(x)\n",
    "    if (is_number(x) and float(x)==0) or (remove_extra_characters(x).casefold() in ('false', 'f', 'no', 'n')):\n",
    "        return 0\n",
    "    elif (is_number(x) and float(x)==1) or (remove_extra_characters(x).casefold() in ('true', 't', 'yes', 'y')):\n",
    "        return 1\n",
    "    else:\n",
    "        return float('nan')\n",
    "\n",
    "def get_label(string, allow_missing=False):\n",
    "    label, has_label = get_variable(string, label_string)\n",
    "    if not has_label and not allow_missing:\n",
    "        raise Exception('No label is available: are you trying to load the labels from the held-out data?')\n",
    "    label = sanitize_boolean_value(label)\n",
    "    return label\n",
    "\n",
    "def load_label(record):\n",
    "    header = load_header(record)\n",
    "    label = get_label(header)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chagas_label(text_extracted_from_signal):\n",
    "    for comment in text_extracted_from_signal['comments']:\n",
    "        if comment.startswith('Chagas label:'):\n",
    "            label_str = comment.split(':')[-1].strip()  # Extract \"False\" or \"True\"\n",
    "            break\n",
    "    true_label = 1 if label_str == \"True\" else 0\n",
    "    return true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(MODEL_PATH)\n",
    "ecg , text= load_signals(RECORD_PATH)\n",
    "\n",
    "fs = int(text[\"fs\"])\n",
    "true_label = get_chagas_label(text)\n",
    "\n",
    "conv_layer_name = find_last_conv_layer(model)\n",
    "original_signal = ecg\n",
    "original_fs = int(text[\"fs\"])\n",
    "if original_signal.ndim != 2 or original_signal.shape[1] != 12:\n",
    "        raise ValueError(\"ECG signal must have shape (samples, 12 leads).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    # Resample and process signal\n",
    "    resampled_signal = resample_signal(original_signal, original_fs, TARGET_FS)\n",
    "    total_samples = resampled_signal.shape[0]\n",
    "    \n",
    "    print(f\"Resampled samples: {total_samples}, Target FS: {TARGET_FS}\")\n",
    "    \n",
    "    adjusted_signal = adjust_signal_length(resampled_signal, REQUIRED_LENGTH)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(\n",
    "        np.expand_dims(adjusted_signal, 0), dtype=tf.float32\n",
    "    )\n",
    "    \n",
    "    # Compute Grad-CAM\n",
    "    heatmap_original, class_idx_original = compute_grad_cam(model, input_tensor, conv_layer_name)\n",
    "    \n",
    "    # Plot Grad-CAM\n",
    "    print(f\"Original Signal - Predicted Class: {class_idx_original}\")\n",
    "    plot_ecg_with_gradcam(\n",
    "        adjusted_signal,\n",
    "        heatmap_original,\n",
    "        text['sig_name'],\n",
    "        TARGET_FS,\n",
    "        class_idx=class_idx_original)\n",
    "        \n",
    "except Exception as main_error:\n",
    "    print(f\"Critical error: {str(main_error)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_records(dataset_path):\n",
    "    records = set()  # to avoid duplicates\n",
    "\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith(\".dat\"):\n",
    "            # Extract the record name (without the .dat extension)\n",
    "            record_name = filename[:-4]\n",
    "            records.add(record_name)\n",
    "\n",
    "    sorted_record_names = sorted(records) # To make the same background using random seed\n",
    "    \n",
    "    return list(sorted_record_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_signal(signal, original_fs, target_length=1000, target_fs=100):\n",
    "    resampled = resample_signal(signal, original_fs, target_fs)\n",
    "    return adjust_signal_length(resampled, target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_background_samples(dataset_path, num_samples=50):\n",
    "    records = load_all_records(dataset_path)\n",
    "    sampled_records = np.random.choice(records, num_samples, replace=False)\n",
    "    background_samples = []\n",
    "    \n",
    "    for record in sampled_records:\n",
    "        record_path = os.path.join(DATASET_DIR, record)\n",
    "        signal, fields = load_signals(record_path)\n",
    "        original_fs = int(fields['fs'])  # Get true sampling rate\n",
    "        processed_signal = preprocess_signal(signal, original_fs)\n",
    "        background_samples.append(processed_signal)\n",
    "    \n",
    "    return np.array(background_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogitModel(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        # Reconstruct the complete computation graph\n",
    "        self.input_layer = model.input\n",
    "        self.feature_extractor = tf.keras.Model(\n",
    "            inputs=model.input,\n",
    "            outputs=model.layers[-2].output\n",
    "        )\n",
    "        \n",
    "        # Explicitly create output layer with original weights (no activation)\n",
    "        self.final_dense = tf.keras.layers.Dense(\n",
    "            units=model.layers[-1].units,\n",
    "            activation=None,\n",
    "            use_bias=True,\n",
    "            kernel_initializer=tf.constant_initializer(model.layers[-1].kernel.numpy()),\n",
    "            bias_initializer=tf.constant_initializer(model.layers[-1].bias.numpy())\n",
    "        )\n",
    "        \n",
    "        # Full computation graph\n",
    "        self.logits = self.final_dense(self.feature_extractor(self.input_layer))\n",
    "        \n",
    "        # Create formal Model with defined inputs/outputs\n",
    "        self.logit_model = tf.keras.Model(\n",
    "            inputs=self.input_layer,\n",
    "            outputs=self.logits\n",
    "        )\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.logit_model(x)\n",
    "    \n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return self.logit_model.inputs\n",
    "    \n",
    "    @property\n",
    "    def outputs(self):\n",
    "        return self.logit_model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shap_explanations(model, adjusted_signal, lead_names, fs, class_idx):\n",
    "    # Load background data\n",
    "    background_samples = load_background_samples(DATASET_DIR)\n",
    "    background = np.array(background_samples).astype(np.float32)\n",
    "    \n",
    "    # Verify background shape\n",
    "    print(\"Background shape:\", background.shape)  # Should be (50, 1000, 12)\n",
    "\n",
    "    wrapped_model = LogitModel(model)\n",
    "    \n",
    "    # Initialize SHAP GradientExplainer\n",
    "    explainer = shap.GradientExplainer(\n",
    "        wrapped_model,\n",
    "        background,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Prepare input signal\n",
    "    input_signal = adjusted_signal[np.newaxis, :, :].astype(np.float32)\n",
    "    print(\"Input signal shape:\", input_signal.shape)  # Should be (1, 1000, 12)\n",
    "    \n",
    "    # Compute SHAP values\n",
    "    shap_values = explainer.shap_values(input_signal)\n",
    "    \n",
    "    # Process and plot\n",
    "    shap_heatmap = np.squeeze(shap_values[0])\n",
    "    shap_heatmap_normalized = (shap_heatmap - np.min(shap_heatmap)) / (np.ptp(shap_heatmap) + 1e-8)\n",
    "    plot_ecg_with_shap(adjusted_signal, shap_heatmap_normalized, lead_names, fs, class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ecg_with_shap(signal, heatmap, lead_names, fs, class_idx):\n",
    "    if heatmap.shape != signal.shape:\n",
    "        raise ValueError(f\"Heatmap shape {heatmap.shape} doesn't match signal {signal.shape}\")\n",
    "    \n",
    "    num_leads = signal.shape[1]\n",
    "    time = np.arange(signal.shape[0])/fs\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = GridSpec(num_leads, 1, figure=fig)\n",
    "    \n",
    "    for lead in range(num_leads):\n",
    "        ax = fig.add_subplot(gs[lead, 0])\n",
    "        offset = lead * 3\n",
    "        ax.plot(time, signal[:, lead] + offset, 'k', lw=1)\n",
    "        ax.imshow(np.expand_dims(heatmap[:, lead], 0),\n",
    "                  cmap='viridis',\n",
    "                  aspect='auto',\n",
    "                  extent=[time[0], time[-1], offset-2, offset+5],\n",
    "                  alpha=0.4,\n",
    "                  origin='lower')\n",
    "        ax.set(ylabel=lead_names[lead], yticks=[], ylim=[offset-2, offset+2])\n",
    "        if lead != num_leads-1:\n",
    "            ax.set_xticks([])\n",
    "    \n",
    "    plt.suptitle(f\"Class {class_idx} - SHAP Values\", y=0.95)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model input name:\", model.input.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_shap_explanations(\n",
    "    model=model,\n",
    "    adjusted_signal=adjusted_signal,\n",
    "    lead_names=text['sig_name'],\n",
    "    fs=TARGET_FS,\n",
    "    class_idx=class_idx_original\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21",
   "metadata": {},
   "source": [
    "Fast Gradient Signed Method (FGSM) attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_output = model.predict(input_tensor)\n",
    "\n",
    "print(\"Predicted probabilities:\", probability_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i in range(12):\n",
    "    plt.subplot(12,1,i+1)\n",
    "    plt.plot(np.asarray(adjusted_signal)[:,i], label= \"Original ECG\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_linear(grad, eps):\n",
    "\n",
    "    axis = list(range(1, len(grad.get_shape())))\n",
    "    avoid_zero_div = 1e-12\n",
    "\n",
    "    # Take sign of gradient\n",
    "    optimal_perturbation = tf.sign(grad)\n",
    "    optimal_perturbation = tf.stop_gradient(optimal_perturbation)\n",
    "    scaled_perturbation = tf.multiply(eps, optimal_perturbation)\n",
    "    \n",
    "    return scaled_perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 100\n",
    "eps = 0.01\n",
    "target_class = 1 - true_label   # Flip the label\n",
    "adv_x = tf.identity(input_tensor)\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    current_prob = model.predict(adv_x)[0][0]\n",
    "    print(current_prob)\n",
    "    \n",
    "    # Success criteria for probabilistic outputs\n",
    "    if (target_class == 0 and current_prob < 0.01) or \\\n",
    "       (target_class == 1 and current_prob > 0.5):\n",
    "        print(f\"Success at iteration {iteration}: Prob={current_prob:.4f}\")\n",
    "        break\n",
    "\n",
    "    # Gradient calculation\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(adv_x)\n",
    "        pred = model(adv_x)\n",
    "        loss = tf.keras.losses.binary_crossentropy(\n",
    "            tf.constant([[target_class]], dtype=tf.float32), \n",
    "            pred\n",
    "        )\n",
    "        loss = -loss  # Gradient ASCENT for adversarial attack\n",
    "\n",
    "    grads = tape.gradient(loss, adv_x)\n",
    "    perturbation = optimize_linear(grads, eps)\n",
    "\n",
    "    perturbation_filtered = np.array([\n",
    "        bandpass_filter(perturbation[0,:,i], fs=TARGET_FS, lowcut=0.5, highcut=49.9)\n",
    "        for i in range(12)\n",
    "    ])\n",
    "    perturbation_filtered = np.expand_dims(perturbation_filtered.T, 0)\n",
    "\n",
    "    adv_x = adv_x + perturbation_filtered\n",
    "    adv_x = tf.clip_by_value(adv_x, np.min(adjusted_signal), np.max(adjusted_signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i in range(12):\n",
    "    plt.subplot(12,1,i+1)\n",
    "    plt.plot(np.asarray(input_tensor)[0,:,i], label= \"Original ECG\")\n",
    "    plt.plot(np.asarray(adv_x)[0,:,i], label= \"Adverserial ECG\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27",
   "metadata": {},
   "source": [
    "Attack Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_pred = model.predict(input_tensor)\n",
    "print(\"Original Prediction:\", original_pred)\n",
    "print(true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"true_label: {true_label}, type: {type(true_label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the original model\n",
    "wrapped_model = LogitModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_output = model(input_tensor)\n",
    "print(\"Original model output (probabilities):\", original_output)\n",
    "\n",
    "\n",
    "wrapped_output = wrapped_model(input_tensor)\n",
    "print(\"Wrapped model output (logits):\", wrapped_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Binary Loss Function\n",
    "def binary_loss(labels, logits):\n",
    "    return tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=labels,  # Shape (1, 1)\n",
    "        logits=logits   # Shape (1, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack parameters\n",
    "target_class = 1 - true_label\n",
    "target_label_tf = tf.constant([[target_class]], dtype=tf.float32)\n",
    "\n",
    "eps = 0.1  # perturbation size\n",
    "alpha = eps / 40  # step size\n",
    "steps = 50  # attack steps\n",
    "\n",
    "# Check gradient flow\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(input_tensor)\n",
    "    logits = wrapped_model(input_tensor)\n",
    "grads = tape.gradient(logits, input_tensor)\n",
    "grad_norm = tf.reduce_mean(tf.abs(grads)).numpy()\n",
    "print(f\"Gradient Norm: {grad_norm}\")  # Should not be too close to zero\n",
    "\n",
    "if grad_norm < 1e-6:\n",
    "    raise ValueError(\"Gradient vanishing! Check if the model is differentiable.\")\n",
    "\n",
    "# **Random Initialization for PGD**\n",
    "random_noise = tf.random.uniform(shape=tf.shape(input_tensor), minval=-eps, maxval=eps)\n",
    "pgd_input = tf.clip_by_value(input_tensor + random_noise, 0, 1)  # Ensure valid input range\n",
    "\n",
    "# Generate adversarial examples\n",
    "adv_bim = projected_gradient_descent(\n",
    "    wrapped_model,  # Use wrapped model\n",
    "    input_tensor,\n",
    "    eps,\n",
    "    alpha,\n",
    "    steps,\n",
    "    norm=np.inf,\n",
    "    y=target_label_tf,\n",
    "    loss_fn=binary_loss  # Use custom binary loss\n",
    ")\n",
    "\n",
    "adv_pgd = projected_gradient_descent(\n",
    "    wrapped_model,  # Use wrapped model\n",
    "    pgd_input,  # PGD starts from a randomly perturbed input\n",
    "    eps,\n",
    "    alpha,\n",
    "    steps,\n",
    "    norm=np.inf,\n",
    "    y=target_label_tf,\n",
    "    loss_fn=binary_loss  # Use custom binary loss\n",
    ")\n",
    "\n",
    "# Check if adversarial examples are different\n",
    "diff = tf.norm(adv_bim - adv_pgd).numpy()\n",
    "print(f\"Difference between BIM and PGD adversarial examples: {diff}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(12):\n",
    "    plt.subplot(12, 1, i+1)\n",
    "    plt.plot(adjusted_signal[:, i], label=\"Original\")\n",
    "    plt.plot(adv_bim.numpy()[0, :, i], label=\"BIM\")\n",
    "    plt.plot(adv_pgd.numpy()[0, :, i], label=\"PGD\")\n",
    "    plt.ylabel(text['sig_name'][i], rotation=0, ha='right')\n",
    "    plt.yticks([])\n",
    "    if i != 11: plt.xticks([])\n",
    "    plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Predictions\n",
    "print(\"Original:\", model.predict(input_tensor))\n",
    "print(\"BIM:\", model.predict(adv_bim))\n",
    "print(\"PGD:\", model.predict(adv_pgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Below this need to be try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_average_beat(signal_lead, sampling_rate):\n",
    "    \"\"\"Extract and average ECG beats using NeuroKit2.\"\"\"\n",
    "    cleaned = nk.ecg_clean(signal_lead, sampling_rate=sampling_rate)\n",
    "    _, rpeaks = nk.ecg_peaks(cleaned, sampling_rate=sampling_rate)\n",
    "    \n",
    "    if len(rpeaks[\"ECG_R_Peaks\"]) == 0:\n",
    "        raise ValueError(\"No R-peaks detected\")\n",
    "        \n",
    "    segments = nk.ecg_segment(cleaned, rpeaks=rpeaks[\"ECG_R_Peaks\"], sampling_rate=sampling_rate)\n",
    "    \n",
    "    # If only one segment exists, use it; otherwise, exclude 'Segment_0' (if needed)\n",
    "    if len(segments) == 1:\n",
    "        beat_keys = list(segments.keys())\n",
    "    else:\n",
    "        beat_keys = [k for k in segments.keys() if k != 'Segment_0']\n",
    "    \n",
    "    # Fallback: if filtering removes all keys, then use 'Segment_0'\n",
    "    if not beat_keys:\n",
    "        beat_keys = ['Segment_0']\n",
    "    \n",
    "    beats = [segments[k][\"Signal\"] for k in beat_keys]\n",
    "    avg_beat = np.mean(beats, axis=0)\n",
    "    return avg_beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate adversarial examples dictionary\n",
    "attacks = {\n",
    "    \"bim\": adv_bim,\n",
    "    \"pgd\": adv_pgd\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_lead(signal, sampling_rate):\n",
    "    \"\"\"Find lead with most detectable R-peaks\"\"\"\n",
    "    max_peaks = -1\n",
    "    best_lead = 0\n",
    "    for lead in range(signal.shape[1]):\n",
    "        try:\n",
    "            non_zero_indices = np.where(signal[:, lead] != 0)[0]\n",
    "            trimmed = signal[:non_zero_indices[-1]+1, lead]\n",
    "            cleaned = nk.ecg_clean(trimmed, sampling_rate=sampling_rate)\n",
    "            _, rpeaks = nk.ecg_peaks(cleaned, sampling_rate=sampling_rate)\n",
    "            if len(rpeaks[\"ECG_R_Peaks\"]) > max_peaks:\n",
    "                max_peaks = len(rpeaks[\"ECG_R_Peaks\"])\n",
    "                best_lead = lead\n",
    "        except:\n",
    "            continue\n",
    "    return best_lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization and analysis\n",
    "for name, adv_example in attacks.items():\n",
    "    \n",
    "    # Beat extraction and comparison\n",
    "    try:\n",
    "        best_lead = find_best_lead(adjusted_signal, TARGET_FS)\n",
    "        \n",
    "        orig_beat = extract_average_beat(adjusted_signal[:, best_lead], TARGET_FS)\n",
    "        adv_beat = extract_average_beat(adv_example.numpy()[0, :, best_lead], TARGET_FS)\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(orig_beat, label=\"Original Beat\")\n",
    "        plt.plot(adv_beat, label=f\"{name.upper()} Beat\")\n",
    "        plt.title(\"Average Beat Comparison\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Beat comparison failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
